{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读入tools 生成的possible matchings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matchings_path = \"./data/employee.json\"\n",
    "# answers_path = \"employee_ans.json\"\n",
    "matchings_path = \"author.json\"\n",
    "answers_path = \"author_ans.json\"\n",
    "# matchings_path = \"purchase.json\"\n",
    "# answers_path = \"purchase_ans.json\"\n",
    "dataset_name = \"author\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fact import FactSet\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "with open(matchings_path, \"r\") as f:\n",
    "    content = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化 chatgpt 的 tokenzier，用于估计correspondence 的cost func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process 统计，factset 需要的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(content):\n",
    "    import numpy as np\n",
    "    c_set = content[\"correspondence_set\"]\n",
    "    # chatgpt 的tokens 的num list\n",
    "    len_list = [len(encoding.encode(i[0][1]+i[1][1])) for i in c_set]\n",
    "    matchings = content[\"matchings\"]\n",
    "    Views = []\n",
    "    for match in matchings:\n",
    "        view = []\n",
    "        for c in c_set:\n",
    "            if c in match:\n",
    "                view.append(1)\n",
    "            else:\n",
    "                view.append(0)\n",
    "        Views.append(view)\n",
    "    return np.array(Views), np.array(content[\"prob_all\"]), np.array(len_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理后生成 facts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts, prob, len_list = process(content)\n",
    "ex_fact = FactSet(facts=facts, prior_p=prob, ground_true=2, len_list=len_list)\n",
    "random_fact = FactSet(facts=facts, prior_p=prob, ground_true=2, len_list=len_list)\n",
    "brute_fact = FactSet(facts=facts, prior_p=prob, ground_true=2, len_list=len_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## selector to select correspondence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化 selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from query import QuerySelector, BaseQuerySelector, GreedyQuerySelector,RandomQuerySelector\n",
    " # 对应fact1, 3是0.8, 0.\n",
    "query_selector = GreedyQuerySelector()\n",
    "# selection_idxes, sub_facts, h = query_selector.select(ex_fact, 2, accuracy, cost_func=2)\n",
    "random_selector = RandomQuerySelector()\n",
    "base_selector = BaseQuerySelector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM答案dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(answers_path, \"r\") as f:\n",
    "    ans_list = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## params\n",
    "# correspondence num\n",
    "c_len = ex_fact.num_fact()\n",
    "\n",
    "#LLM 预测准确率\n",
    "p_w = 0.9\n",
    "# 计算p(A_T) 和 P(A_T|v) 需要的LLM array:acc \n",
    "acc = np.array([[p_w for i in range(c_len)]])\n",
    "\n",
    "# 每个token 一个\n",
    "budget = 300 # 单位为 price of one token  \n",
    "turns = 20\n",
    "# k-selection 的参数\n",
    "k = 4\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b_turns = 2\n",
    "# brute_h_list = [brute_fact.compute_entropy()]\n",
    "# while b_turns>0:\n",
    "#     brute_selection_idxes, _, _ = base_selector.select(brute_fact, k, acc)\n",
    "#     ans_brute = [1 if ans_list[ix_r]==\"yes\" else 0 for ix_r in brute_selection_idxes]\n",
    "#     p_a_b,p_a_v_b = ex_fact.compute_ans_p(ans_brute, brute_selection_idxes, acc)\n",
    "#     sum_p = []\n",
    "#     for idx,i in enumerate(brute_fact.get_prior_p()):\n",
    "#         p_post_b = i*p_a_v_b[idx]/p_a_b\n",
    "#         sum_p.append(p_post_b)\n",
    "#     brute_fact.set_prior_p(np.array(sum_p))\n",
    "#     ex_fact.set_prior_p(np.array(sum_p))\n",
    "#     random_fact.set_prior_p(np.array(sum_p))\n",
    "#     brute_h_list.append(brute_fact.compute_entropy())\n",
    "#     b_turns-=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "approx_h_list=[ex_fact.compute_entropy()]\n",
    "\n",
    "\n",
    "cost_sum = 0\n",
    "while turns>0:\n",
    "    selection_idxes, sub_facts, h = query_selector.select(ex_fact, k, acc, cost_func=1)\n",
    "    # for ix in selection_idxes:\n",
    "    #     cost_sum += ex_fact.len_list()[ix]\n",
    "    #     ans=[1 if ans_list[ix]==\"yes\" else 0]\n",
    "    #     p_a, p_a_v = ex_fact.compute_ans_p(ans, [ix], acc)\n",
    "    #     sum_p = []\n",
    "    #     for idx,i in enumerate(ex_fact.get_prior_p()):\n",
    "    #         p_post = i*p_a_v[idx]/p_a\n",
    "    #         sum_p.append(p_post)\n",
    "    #     ex_fact.set_prior_p(np.array(sum_p))\n",
    "    ans = [1 if ans_list[ix_r]==\"yes\" else 0 for ix_r in selection_idxes]\n",
    "    p_a,p_a_v = ex_fact.compute_ans_p(ans, selection_idxes, acc)\n",
    "    sum_p = []\n",
    "    for idx,i in enumerate(ex_fact.get_prior_p()):\n",
    "        p_post = i*p_a_v[idx]/p_a\n",
    "        sum_p.append(p_post)\n",
    "    ex_fact.set_prior_p(np.array(sum_p))\n",
    "    approx_h_list.append(ex_fact.compute_entropy())\n",
    "    turns -=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_h_list = []\n",
    "cost_list = []\n",
    "p_prior = random_fact.get_prior_p()\n",
    "for _ in range(100):\n",
    "    cost_sum_r = 0\n",
    "    random_fact.set_prior_p(p_prior)\n",
    "    turns_r=20\n",
    "    random_h_list=[random_fact.compute_entropy()]\n",
    "    random_selection_idxes, _, _  =  random_selector.select(random_fact, k, acc)\n",
    "    while turns_r>0:\n",
    "        # for ix_r in random_selection_idxes:\n",
    "        #     cost_sum_r += ex_fact.len_list()[ix]\n",
    "        #     ans_r = [1 if ans_list[ix_r]==\"yes\" else 0]\n",
    "        #     p_a_r, p_a_v_r = random_fact.compute_ans_p(ans_r, [ix_r], acc)\n",
    "        #     sum_p_r = []\n",
    "        #     for idx,i in enumerate(random_fact.get_prior_p()):\n",
    "        #         p_post_r = i*p_a_v_r[idx]/p_a_r\n",
    "        #         sum_p_r.append(p_post_r)\n",
    "        #     random_fact.set_prior_p(np.array(sum_p_r))\n",
    "        ans_r = [1 if ans_list[ix_r]==\"yes\" else 0 for ix_r in random_selection_idxes]\n",
    "        p_a_r,p_a_v_r = random_fact.compute_ans_p(ans_r, random_selection_idxes, acc)\n",
    "    \n",
    "        sum_p_r = []\n",
    "        for idx,i in enumerate(random_fact.get_prior_p()):\n",
    "            p_post_r = i*p_a_v_r[idx]/p_a_r\n",
    "            sum_p_r.append(p_post_r)\n",
    "        random_fact.set_prior_p(np.array(sum_p_r))\n",
    "        random_h_list.append(random_fact.compute_entropy())\n",
    "        turns_r -=1\n",
    "    all_h_list.append(random_h_list)\n",
    "    cost_list.append(cost_sum_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0179569806623436,\n",
       " 0.6690536564277565,\n",
       " 0.12436480210466297,\n",
       " 0.019241796784894613,\n",
       " 0.0027203962486460308,\n",
       " 0.00036375588156893146,\n",
       " 4.7475444288694216e-05,\n",
       " 6.062951740463603e-06,\n",
       " 7.612620892531361e-07,\n",
       " 9.431892306677457e-08,\n",
       " 1.1561474991512041e-08,\n",
       " 1.4047856462938792e-09,\n",
       " 1.69440139271705e-10,\n",
       " 2.0310254887040417e-11,\n",
       " 2.421473354359302e-12,\n",
       " 2.873695262432069e-13,\n",
       " 3.3928154516766186e-14,\n",
       " 3.897243409737681e-15,\n",
       " 4.581531515828801e-16,\n",
       " 5.369769529326141e-17,\n",
       " 6.276609428343524e-18]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approx_h_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.01795698, 0.77266288, 0.51744036, 0.47412442, 0.46755725,\n",
       "        0.46663131, 0.4665066 , 0.46649032, 0.46648824, 0.46648798,\n",
       "        0.46648794, 0.46648794, 0.46648794, 0.46648794, 0.46648794,\n",
       "        0.46648794, 0.46648794, 0.46648794, 0.46648794, 0.46648794,\n",
       "        0.46648794]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "n = np.array(all_h_list)\n",
    "random_h_l = n.mean(axis=0, keepdims=True)\n",
    "random_h_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "dataset = \"output\"\n",
    "def save_h_file(approx_name_f,random_f, approx_ob, random_ob):\n",
    "    with open(f\"./{dataset}/\"+approx_name_f, \"w\") as f:\n",
    "        json.dump(approx_ob, f, indent=2, ensure_ascii=False)\n",
    "    with open(f\"./{dataset}/\"+random_f, \"w\") as w:\n",
    "        json.dump(random_ob, w, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"turns_20_k={}_{}\"\n",
    "save_h_file(name.format(k,f\"{dataset_name}_approx.json\"), name.format(k,f\"{dataset_name}_random.json\"),approx_h_list, random_h_l.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "2f394aca7ca06fed1e6064aef884364492d7cdda3614a461e02e6407fc40ba69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
