{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读入tools 生成的possible matchings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matchings_path = \"employee.json\"\n",
    "# answers_path = \"employee_ans.json\"\n",
    "matchings_path = \"./dataset/author.json\"\n",
    "answers_path = \"./dataset/author_ans.json\"\n",
    "# matchings_path = \"purchase.json\"\n",
    "# answers_path = \"purchase_ans.json\"\n",
    "dataset_name = \"author\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fact import FactSet\n",
    "from llm_api import gpt_check\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "with open(matchings_path, \"r\") as f:\n",
    "    content = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化 chatgpt 的 tokenzier，用于估计correspondence 的cost func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process 统计，factset 需要的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(content):\n",
    "    import numpy as np\n",
    "    c_set = content[\"correspondence_set\"]\n",
    "    # chatgpt 的tokens 的num list\n",
    "    len_list = [len(encoding.encode(i[0][1]+i[1][1])) for i in c_set]\n",
    "    matchings = content[\"matchings\"]\n",
    "    Views = []\n",
    "    for match in matchings:\n",
    "        view = []\n",
    "        for c in c_set:\n",
    "            if c in match:\n",
    "                view.append(1)\n",
    "            else:\n",
    "                view.append(0)\n",
    "        Views.append(view)\n",
    "    return np.array(Views), np.array(content[\"prob_all\"]), np.array(len_list), max(len_list), sum(len_list), c_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理后生成 facts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_cost:  6 note that budget of each round must > 18 and < 51:\n"
     ]
    }
   ],
   "source": [
    "facts, prob, len_list, max_cost, sum_c, c_set = process(content)\n",
    "print(\"max_cost: \", max_cost, f\"note that budget of each round must > {3*max_cost} and < {sum_c}:\")\n",
    "ex_fact = FactSet(facts=facts, prior_p=prob, ground_true=2, len_list=len_list)\n",
    "random_fact = FactSet(facts=facts, prior_p=prob, ground_true=2, len_list=len_list)\n",
    "brute_fact = FactSet(facts=facts, prior_p=prob, ground_true=2, len_list=len_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## selector to select correspondence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化 selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "from query import QuerySelector, BaseQuerySelector, GreedyQuerySelector,RandomQuerySelector\n",
    " # 对应fact1, 3是0.8, 0.\n",
    "query_selector = GreedyQuerySelector()\n",
    "# selection_idxes, sub_facts, h = query_selector.select(ex_fact, 2, accuracy, cost_func=2)\n",
    "random_selector = RandomQuerySelector()\n",
    "base_selector = BaseQuerySelector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM答案dict\n",
    "## 由于数量问题，模拟实验，从web chatgpt端，得到所有的correpspondence 验证答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(answers_path, \"r\") as f:\n",
    "    ans_list = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "## params\n",
    "# correspondence num\n",
    "\n",
    "api_use = False \n",
    "c_len = ex_fact.num_fact()\n",
    "\n",
    "#LLM 预测准确率\n",
    "p_w = 0.9\n",
    "# 计算p(A_T) 和 P(A_T|v) 需要的LLM array:acc \n",
    "acc = np.array([[p_w for i in range(c_len)]])\n",
    "\n",
    "# 每个token 一个\n",
    "budget = 40 # 单位为 price of one token  \n",
    "turns = 20\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "approx_h_list=[ex_fact.compute_entropy()]\n",
    "\n",
    "cost_sum = 0\n",
    "while turns>0:\n",
    "    selection_idxes, sub_facts, h = query_selector.select(ex_fact, budget, acc, cost_func=2)\n",
    "    if api_use:\n",
    "         ans = [1 if gpt_check(ix_r, c_set)==\"yes\" else 0 for ix_r in selection_idxes]\n",
    "    else:\n",
    "        ans = [1 if ans_list[ix_r]==\"yes\" else 0 for ix_r in selection_idxes]\n",
    "    p_a,p_a_v = ex_fact.compute_ans_p(ans, selection_idxes, acc)\n",
    "    sum_p = []\n",
    "    for idx,i in enumerate(ex_fact.get_prior_p()):\n",
    "        p_post = i*p_a_v[idx]/p_a\n",
    "        sum_p.append(p_post)\n",
    "    ex_fact.set_prior_p(np.array(sum_p))\n",
    "    approx_h_list.append(ex_fact.compute_entropy())\n",
    "    turns -=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_h_list = []\n",
    "cost_list = []\n",
    "p_prior = random_fact.get_prior_p()\n",
    "for _ in range(100):\n",
    "    cost_sum_r = 0\n",
    "    random_fact.set_prior_p(p_prior)\n",
    "    turns_r=20\n",
    "    random_h_list=[random_fact.compute_entropy()]\n",
    "    random_selection_idxes, _, _  =  random_selector.select(random_fact, budget, acc)\n",
    "    while turns_r>0:\n",
    "        # for ix_r in random_selection_idxes:\n",
    "        #     cost_sum_r += ex_fact.len_list()[ix]\n",
    "        #     ans_r = [1 if ans_list[ix_r]==\"yes\" else 0]\n",
    "        #     p_a_r, p_a_v_r = random_fact.compute_ans_p(ans_r, [ix_r], acc)\n",
    "        #     sum_p_r = []\n",
    "        #     for idx,i in enumerate(random_fact.get_prior_p()):\n",
    "        #         p_post_r = i*p_a_v_r[idx]/p_a_r\n",
    "        #         sum_p_r.append(p_post_r)\n",
    "        #     random_fact.set_prior_p(np.array(sum_p_r))\n",
    "        if api_use:\n",
    "            ans_r = [1 if gpt_check(ix_r, c_set)==\"yes\" else 0 for ix_r in random_selection_idxes]\n",
    "        else:\n",
    "            ans_r = [1 if ans_list[ix_r]==\"yes\" else 0 for ix_r in random_selection_idxes]\n",
    "        p_a_r,p_a_v_r = random_fact.compute_ans_p(ans_r, random_selection_idxes, acc)\n",
    "    \n",
    "        sum_p_r = []\n",
    "        for idx,i in enumerate(random_fact.get_prior_p()):\n",
    "            p_post_r = i*p_a_v_r[idx]/p_a_r\n",
    "            sum_p_r.append(p_post_r)\n",
    "        random_fact.set_prior_p(np.array(sum_p_r))\n",
    "        random_h_list.append(random_fact.compute_entropy())\n",
    "        turns_r -=1\n",
    "    all_h_list.append(random_h_list)\n",
    "    cost_list.append(cost_sum_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0179569806623436,\n",
       " 0.06647723876515912,\n",
       " 0.00020312553145128786,\n",
       " 3.4248007772491767e-06,\n",
       " 5.362396693911432e-08,\n",
       " 9.689744258836293e-11,\n",
       " 1.64961660325674e-13,\n",
       " 2.239756002390407e-15,\n",
       " 3.090440380925279e-17,\n",
       " 4.216975178650379e-19,\n",
       " 5.701965262137485e-21,\n",
       " 7.651590528901612e-23,\n",
       " 1.0202120705202144e-24,\n",
       " 1.3528189321027066e-26,\n",
       " 1.7853293697396283e-28,\n",
       " 2.3463109996578138e-30,\n",
       " 3.072236625066345e-32,\n",
       " 4.009620992150077e-34,\n",
       " 5.9461253730027415e-37,\n",
       " 7.707940298336882e-39,\n",
       " 9.969117375861922e-41]"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approx_h_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.01795698, 0.32667247, 0.18802263, 0.16491988, 0.16134928,\n",
       "        0.16083871, 0.16076929, 0.16076016, 0.16075899, 0.16075885,\n",
       "        0.16075883, 0.16075882, 0.16075882, 0.16075882, 0.16075882,\n",
       "        0.16075882, 0.16075882, 0.16075882, 0.16075882, 0.16075882,\n",
       "        0.16075882]])"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "n = np.array(all_h_list)\n",
    "random_h_l = n.mean(axis=0, keepdims=True)\n",
    "random_h_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "dataset = \"output\"\n",
    "def save_h_file(approx_name_f,random_f, approx_ob, random_ob):\n",
    "    with open(f\"./{dataset}/\"+approx_name_f, \"w\") as f:\n",
    "        json.dump(approx_ob, f, indent=2, ensure_ascii=False)\n",
    "    with open(f\"./{dataset}/\"+random_f, \"w\") as w:\n",
    "        json.dump(random_ob, w, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"turns_20_budget={}_{}\"\n",
    "save_h_file(name.format(budget,f\"{dataset_name}_approx.json\"), name.format(budget,f\"{dataset_name}_random.json\"),approx_h_list, random_h_l.tolist()[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "2f394aca7ca06fed1e6064aef884364492d7cdda3614a461e02e6407fc40ba69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
